{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T06:12:38.936688Z",
     "start_time": "2018-03-23T06:12:38.533915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda? False\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.LongTensor' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/StageL3/Regard.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ajout de la constante de temps t0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ajout de la constante de temps t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/StageL3/Regard.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test set: Final Accuracy: {:.3f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print que le pourcentage de rÃ©ussite final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/StageL3/Regard.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#loss = F.nll_loss(output, target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[1;32m    679\u001b[0m                                self.ignore_index, self.reduce)\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m_assert_no_grad\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"nn criterions don't compute the gradient w.r.t. targets - please \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"mark these variables as volatile or not requiring gradients\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.LongTensor' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "%run Regard.py --no-cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T06:14:01.543835Z",
     "start_time": "2018-03-23T06:14:01.408515Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "import argparse\r\n",
      "import time\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "import torch.nn.functional as F\r\n",
      "import torch.optim as optim\r\n",
      "from torch.autograd import Variable\r\n",
      "import torch.nn.parallel\r\n",
      "import torch.backends.cudnn as cudnn\r\n",
      "import torch.distributed as dist\r\n",
      "import torch.optim\r\n",
      "import torch.utils.data\r\n",
      "import torch.utils.data.distributed\r\n",
      "import torchvision.transforms as transforms\r\n",
      "import torchvision.datasets as datasets\r\n",
      "import torchvision.models as models\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "import torchvision\r\n",
      "\r\n",
      "\r\n",
      "# Training settings\r\n",
      "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\r\n",
      "parser.add_argument('--batch-size', type=int, default=10, metavar='N',\r\n",
      "                    help='input batch size for training (default: 64)')\r\n",
      "parser.add_argument('--test-batch-size', type=int, default=10, metavar='N',\r\n",
      "                    help='input batch size for testing (default: 1000)')\r\n",
      "parser.add_argument('--epochs', type=int, default=10, metavar='N',\r\n",
      "                    help='number of epochs to train (default: 10)')\r\n",
      "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\r\n",
      "                    help='learning rate (default: 0.01)')\r\n",
      "parser.add_argument('--momentum', type=float, default=0.9, metavar='M', #default = 0.5\r\n",
      "                    help='SGD momentum (default: 0.5)')\r\n",
      "parser.add_argument('--no-cuda', action='store_true', default=False,\r\n",
      "                    help='disables CUDA training')\r\n",
      "parser.add_argument('--seed', type=int, default=1, metavar='S',\r\n",
      "                    help='random seed (default: 1)')\r\n",
      "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\r\n",
      "                    help='how many batches to wait before logging training status')\r\n",
      "parser.add_argument('--dimension', type=int, default = 120, metavar='D',\r\n",
      "                    help='the dimension of the second neuron network') #ajout de l'argument dimension reprÃ©sentant le nombre de neurone dans la deuxiÃ¨me couche. \r\n",
      "parser.add_argument('--boucle', type=int, default=0, metavar='B',\r\n",
      "                   help='boucle pour faire diffÃ©rents couche de la deuxiÃ¨me couche de neurone')# ajout de boucle pour automatiser le nombre de neurone dans la deuxieme couche\r\n",
      "args = parser.parse_args()\r\n",
      "args.cuda = not args.no_cuda and torch.cuda.is_available()\r\n",
      "print ('cuda?', args.cuda)\r\n",
      "torch.manual_seed(args.seed)\r\n",
      "if args.cuda:\r\n",
      "    torch.cuda.manual_seed(args.seed)\r\n",
      "\r\n",
      "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\r\n",
      "dataset = torchvision.datasets.ImageFolder('dataset',\r\n",
      "                                        transforms.Compose([\r\n",
      "                                        transforms.CenterCrop(150),\r\n",
      "                                        transforms.ToTensor(),\r\n",
      "                                       transforms.Normalize((0.1307,), (0.3081,))\r\n",
      "                        \r\n",
      "                ])) \r\n",
      "train_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\r\n",
      "test_loader= torch.utils.data.DataLoader(dataset, batch_size=args.test_batch_size, shuffle=True, num_workers=2)\r\n",
      "\r\n",
      "classes = 'blink','left','right','center'\r\n",
      "\r\n",
      "class Net(nn.Module):\r\n",
      "    def __init__(self):\r\n",
      "        super(Net, self).__init__()\r\n",
      "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\r\n",
      "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n",
      "        self.conv2_drop = nn.Dropout2d()\r\n",
      "        self.fc1 = nn.Linear(12800, args.dimension)# args.dimension prend la valeur default de l'argument dimension.\r\n",
      "        self.fc2 = nn.Linear(args.dimension, 84)\r\n",
      "        self.fc3 = nn.Linear(84, 10)\r\n",
      "        \r\n",
      "    def forward(self, x):\r\n",
      "        x = F.relu(F.max_pool2d(self.conv1(x), 4))\r\n",
      "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 4))\r\n",
      "        x = x.view(-1, 12800)\r\n",
      "        x = F.relu(self.fc1(x))\r\n",
      "        x = F.relu(self.fc2(x))#\r\n",
      "        x = F.dropout(x, training=self.training)\r\n",
      "        x = self.fc3(x)#\r\n",
      "        #x = self.fc2(x)\r\n",
      "        return F.log_softmax(x, dim=1)#x\r\n",
      "\r\n",
      "model = Net()\r\n",
      "if args.cuda:\r\n",
      "    model.cuda()\r\n",
      "\r\n",
      "criterion = nn.CrossEntropyLoss()#\r\n",
      "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n",
      "\r\n",
      "def train(epoch):\r\n",
      "    model.train()\r\n",
      "    for batch_idx, (data, target) in enumerate(train_loader,0):\r\n",
      "        if args.cuda:\r\n",
      "            data, target = data.cuda(), target.cuda()\r\n",
      "        data, target = Variable(data), Variable(target)\r\n",
      "        optimizer.zero_grad()\r\n",
      "        output = model(data)\r\n",
      "        #loss = F.nll_loss(output, target)\r\n",
      "        loss = criterion(output, labels)#\r\n",
      "        loss.backward()\r\n",
      "        optimizer.step()\r\n",
      "        if args.log_interval>0: # rajout de la commande pour pouvoir print ou non les diffÃ©rents epoch ou juste le rÃ©sultat\r\n",
      "            if batch_idx % args.log_interval == 0:\r\n",
      "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
      "                    epoch, batch_idx * len(data), len(train_loader.dataset),\r\n",
      "                    100. * batch_idx / len(train_loader), loss.data[0]))\r\n",
      "\r\n",
      "dataiter = iter(train_loader)\r\n",
      "images, labels = dataiter.next()\r\n",
      "\r\n",
      "def test():\r\n",
      "    model.eval()\r\n",
      "    test_loss = 0\r\n",
      "    correct = 0\r\n",
      "    for data, target in test_loader:\r\n",
      "        if args.cuda:\r\n",
      "            data, target = data.cuda(), target.cuda()\r\n",
      "        data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "        output = model(data)\r\n",
      "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\r\n",
      "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n",
      "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\r\n",
      "\r\n",
      "    test_loss /= len(test_loader.dataset)\r\n",
      "    if args.log_interval>0: print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
      "        test_loss, correct, len(test_loader.dataset),\r\n",
      "        100. * correct / len(test_loader.dataset)))\r\n",
      "    return correct / len(test_loader.dataset)\r\n",
      "\r\n",
      "def protocol():\r\n",
      "    for epoch in range(1, args.epochs + 1):\r\n",
      "        train(epoch)\r\n",
      "    return test()\r\n",
      "\r\n",
      "def main():\r\n",
      "    for epoch in range(1, args.epochs + 1):\r\n",
      "        train(epoch)\r\n",
      "        Accuracy = test()\r\n",
      "    print('Test set: Final Accuracy: {:.3f}%'.format(Accuracy*100)) # print que le pourcentage de rÃ©ussite final\r\n",
      "    \r\n",
      "    \r\n",
      "if __name__ == '__main__':  \r\n",
      "    if args.boucle == 1: # Pour que la boucle se fasse indiquer --boucle 1\r\n",
      "        rho = 10**(1/3) \r\n",
      "        for i in [int (k) for k in rho**np.arange(2,9)]:# i prend les valeur en entier du tuple rho correspondra au nombre de neurone\r\n",
      "            args.dimension = i\r\n",
      "            print ('La deuxiÃ¨me couche de neurone comporte',i,'neurones')\r\n",
      "            main()\r\n",
      "    else:\r\n",
      "        t0 = time.time () # ajout de la constante de temps t0\r\n",
      "\r\n",
      "        main()\r\n",
      "\r\n",
      "        t1 = time.time () # ajout de la constante de temps t1\r\n",
      "\r\n",
      "        print (\"Le programme a mis\",t1-t0, \"secondes Ã  s'exÃ©cuter.\") #compare t1 et t0, connaitre le temps d'execution du programme"
     ]
    }
   ],
   "source": [
    "%cat Regard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T08:06:23.613342Z",
     "start_time": "2018-03-23T08:06:22.862363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 100, 100]) torch.Size([10])\n",
      "x= torch.Size([10, 3, 100, 100])\n",
      "x1= torch.Size([10, 10, 24, 24])\n",
      "x2= torch.Size([10, 20, 5, 5])\n",
      "x3= torch.Size([10, 500])\n",
      "x4= torch.Size([10, 120])\n",
      "x5= torch.Size([10, 120])\n",
      "x6= torch.Size([10, 4])\n",
      "data= torch.Size([10, 3, 100, 100])\n",
      "output_max= (Variable containing:\n",
      "-1.1345\n",
      "-0.8863\n",
      "-0.9773\n",
      "-1.2427\n",
      "-1.3064\n",
      "-1.3204\n",
      "-1.3068\n",
      "-1.1069\n",
      "-1.2023\n",
      "-1.1904\n",
      "[torch.FloatTensor of size 10]\n",
      ", Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.LongTensor of size 10]\n",
      ")\n",
      "target= Variable containing:\n",
      " 4\n",
      " 3\n",
      " 4\n",
      " 3\n",
      " 2\n",
      " 4\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.LongTensor of size 10]\n",
      " <class 'torch.autograd.variable.Variable'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "log_softmax(): argument 'input' (position 1) must be Variable, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f645e7a38506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ajout de la constante de temps t0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ajout de la constante de temps t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f645e7a38506>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mAccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test set: Final Accuracy: {:.3f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print que le pourcentage de rÃ©ussite final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f645e7a38506>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#loss = F.nll_loss(output, target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 679\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: log_softmax(): argument 'input' (position 1) must be Variable, not tuple"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Training settings\n",
    "cuda = False\n",
    "torch.manual_seed(42)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder('dataset',\n",
    "                                        transforms.Compose([\n",
    "                                        transforms.CenterCrop(100),\n",
    "                                        transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                        ])) \n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "test_loader= torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, args.dimension)# args.dimension prend la valeur default de l'argument dimension.\n",
    "        self.fc2 = nn.Linear(args.dimension, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('x=', x.shape)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 4))\n",
    "        print('x1=', x.shape)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 4))\n",
    "        print('x2=', x.shape)\n",
    "        x = x.view(-1, 500)\n",
    "        print('x3=', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print('x4=', x.shape)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        print('x5=', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print('x6=', x.shape)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()#\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):#,0):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target) #[batch_idx].unsqueeze(0)\n",
    "        print(data.size(), target.size())\n",
    "              \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)#\n",
    "        output_max = torch.max(output,1)\n",
    "        print('data=',data.shape)\n",
    "        print ('output_max=',output_max)\n",
    "        \n",
    "        #print ('indent=',indent,type(indent))\n",
    "        print ('target=',target,type(target))\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        \n",
    "        loss = criterion(output_max, target)#\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if args.log_interval>0: # rajout de la commande pour pouvoir print ou non les diffÃ©rents epoch ou juste le rÃ©sultat\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if args.log_interval>0: print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "def protocol():\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "    return test()\n",
    "\n",
    "def main():\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        Accuracy = test()\n",
    "    print('Test set: Final Accuracy: {:.3f}%'.format(Accuracy*100)) # print que le pourcentage de rÃ©ussite final\n",
    "    \n",
    "    \n",
    "t0 = time.time () # ajout de la constante de temps t0\n",
    "\n",
    "main()\n",
    "\n",
    "t1 = time.time () # ajout de la constante de temps t1\n",
    "\n",
    "print (\"Le programme a mis\",t1-t0, \"secondes Ã  s'exÃ©cuter.\") #compare t1 et t0, connaitre le temps d'execution du programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% run mnist.py --log-interval 0 --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
